{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Model snow depth using terrain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xdem\n",
    "import geoutils as gu\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = '/Users/raineyaberle/Research/PhD/SnowDEMs/skysat_stereo_snow/scripts'\n",
    "data_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/SkySat-Stereo/study-sites'\n",
    "\n",
    "# Inputs\n",
    "site_name = \"MCS\"\n",
    "date = \"20240420\"\n",
    "sd_fn = os.path.join(data_dir, site_name, date, 'post_process', f\"ba+DEMuncertainty1mAll_{site_name}_{date}_DEM_GCPshift_snow_depth.tif\")\n",
    "refdem_fn = os.path.join(data_dir, site_name, 'refdem', f\"{site_name}_REFDEM_WGS84.tif\")\n",
    "# refdem_fn = os.path.join(data_dir, site_name, 'refdem', 'USGS_LPC_ID_FEMAHQ_2018_D18_merged_filtered.tif')\n",
    "terrain_dict_fn = os.path.join(data_dir, site_name, 'refdem', 'refdem_terrain_sampling_resolutions.json')\n",
    "\n",
    "# Outputs\n",
    "out_dir = os.path.join(data_dir, site_name, date, 'snow_depth_modeling')\n",
    "training_data_fn = os.path.join(out_dir, 'training_data.csv')\n",
    "error_fn = os.path.join(out_dir, 'model_error.csv')\n",
    "model_fn = os.path.join(out_dir, 'trained_model.joblib')\n",
    "\n",
    "# Check that inputs exist\n",
    "for fn, name in [[sd_fn, 'Snow depth map'], [refdem_fn, 'Reference DEM'], [terrain_dict_fn, 'Terrain resolutions dictionary']]:\n",
    "    if not os.path.exists(fn):\n",
    "        print(f\"{name} not found, please correct before continuing.\")\n",
    "# Create output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# Import processing functions\n",
    "sys.path.append(code_dir)\n",
    "import post_process_utils as pprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data already exist\n",
    "if not os.path.exists(training_data_fn):\n",
    "    ### Load input files\n",
    "    # Snow depth\n",
    "    sd = xdem.DEM(sd_fn)\n",
    "    # Reference DEM \n",
    "    refdem = xdem.DEM(refdem_fn).reproject(sd)\n",
    "    # Reference DEM sampling resolutions\n",
    "    with open(terrain_dict_fn, 'r') as f:\n",
    "        terrain_dict = json.load(f)\n",
    "    for key, value in terrain_dict.items():\n",
    "        terrain_dict[key] = float(value)\n",
    "    \n",
    "    ### Calculate terrain parameters\n",
    "    elev, slope, aspect, tpi, sx = pprocess.calculate_terrain_params(refdem, terrain_dict, sd)\n",
    "    # Compile into pandas.DataFrame\n",
    "    training_data = pd.DataFrame({'elevation': refdem.data.ravel(),\n",
    "                                  'slope': slope.data.ravel(),\n",
    "                                  'aspect': aspect.data.ravel(),\n",
    "                                  'topographic_position_index': tpi.data.ravel(),\n",
    "                                  'Sx': sx.data.ravel(),\n",
    "                                  'snow_depth': sd.data.ravel()})\n",
    "    training_data.dropna(inplace=True)\n",
    "    training_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Reduce precision (don't need 10 digits of elevation, e.g.)\n",
    "    training_data = training_data.round(2)\n",
    "    \n",
    "    # Save to file\n",
    "    training_data.to_csv(training_data_fn, index=False)\n",
    "    print('Training data saved to file:', training_data_fn)\n",
    "    \n",
    "    # Plot pairplot\n",
    "    # fig_fn = os.path.join(out_dir, 'training_data_pairplot.png')\n",
    "    # fig = sns.pairplot(training_data, corner=True, kind='hist', diag_kind='kde')\n",
    "    # fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    # print('Pairplot saved to file:', fig_fn)\n",
    "    # plt.show()\n",
    "\n",
    "else:\n",
    "\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    print('Training data loaded from file.')\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['elevation', 'slope', 'aspect', 'topographic_position_index', 'Sx']\n",
    "target_cols = ['snow_depth']\n",
    "X = training_data[feature_cols]\n",
    "y = training_data[target_cols].values\n",
    "\n",
    "# Fit a standard scaler to the feature columns\n",
    "scaler_fn = os.path.join(out_dir, 'feature_scaler.joblib')\n",
    "if not os.path.exists(scaler_fn):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    joblib.dump(scaler, scaler_fn)\n",
    "    print('Feature scaler saved to file:', scaler_fn)\n",
    "else:\n",
    "    scaler = joblib.load(scaler_fn)\n",
    "    print('Feature scaler loaded from file.')\n",
    "    \n",
    "# Transform the feature columns\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, and test the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = os.path.join(out_dir, 'trained_model.joblib')\n",
    "if not os.path.exists(model_fn):\n",
    "\n",
    "    ### Use a subset of the training data\n",
    "    X_scaled_sub, y_sub = X_scaled[::100,:], y[::100,:]\n",
    "\n",
    "    best_model = RandomForestRegressor().fit(X_scaled_sub, y_sub)\n",
    "    joblib.dump(best_model, model_fn)\n",
    "    print('Trained model saved to file:', model_fn)\n",
    "    \n",
    "    # ### Set up the hyperparameter grid\n",
    "    # # Number of trees\n",
    "    # n_estimators = [int(x) for x in np.linspace(200, 2000, num = 10)]\n",
    "    # # Number of features at every split\n",
    "    # max_features = ['auto', 'sqrt']\n",
    "    # # Maximum number of levels in tree\n",
    "    # max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    # max_depth.append(None)\n",
    "    # # Minimum number of samples required to split a node\n",
    "    # min_samples_split = [2, 5, 10]\n",
    "    # # Minimum number of samples required at each leaf node\n",
    "    # min_samples_leaf = [1, 2, 4]\n",
    "    # # Create the random grid\n",
    "    # param_grid = {'n_estimators': n_estimators,\n",
    "    #               'max_features': max_features,\n",
    "    #               'max_depth': max_depth,\n",
    "    #               'min_samples_split': min_samples_split,\n",
    "    #               'min_samples_leaf': min_samples_leaf}\n",
    "    # # print('Hyperparameter Grid:')\n",
    "    # # print(json.dumps(random_grid, sort_keys=True, indent=4))\n",
    "\n",
    "    # ### Search for best hyperparameters\n",
    "    # rf = RandomForestRegressor()\n",
    "    # rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "    #                             n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    # rf_random.fit(X_scaled_sub, y_sub)\n",
    "    # print('Best parameters:')\n",
    "    # print(rf_random.best_params_)\n",
    "    \n",
    "    # ### Compare to the default (base) model\n",
    "    # cv_model = rf_random.best_estimator_\n",
    "    # y_pred = cv_model.predict(X_scaled)\n",
    "    # cv_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    # print(f\"Best model RMSE = {np.round(cv_rmse,2)} m\")\n",
    "    # base_model = RandomForestRegressor().fit(X_scaled_sub, y_sub)\n",
    "    # y_pred = base_model.predict(X_scaled)\n",
    "    # base_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    # print(f\"CV model RMSE = {np.round(base_rmse,2)} m\")\n",
    "\n",
    "    # ### Save the better model\n",
    "    # if base_rmse < cv_rmse:\n",
    "    #     # Use base model\n",
    "    #     best_model = base_model\n",
    "    # else:\n",
    "    #     # Use CV model\n",
    "    #     best_model = cv_model\n",
    "    # joblib.dump(best_model, model_fn)\n",
    "    # print('Best model saved to file:', model_fn)\n",
    "\n",
    "else:\n",
    "    best_model = joblib.load(model_fn)\n",
    "    print('Best model loaded from file.')\n",
    "    \n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate feature importances in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_fn = os.path.join(out_dir, 'feature_importances.csv')\n",
    "if not os.path.exists(feature_importances_fn):\n",
    "    # Feature importances via mean decrease in impurity\n",
    "    mdi_importances = best_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in best_model.estimators_], axis=0)\n",
    "    mdi_importances = pd.Series(mdi_importances, index=feature_cols)\n",
    "\n",
    "    # Feature importances via permutation\n",
    "    result = permutation_importance(best_model, X_scaled_sub, y_sub, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_importances = pd.Series(result.importances_mean, index=feature_cols)\n",
    "\n",
    "    # Save in dataframe\n",
    "    feature_importances = pd.DataFrame(mdi_importances.values, feature_cols, ['MDI'])\n",
    "    feature_importances['MDI_std'] = std\n",
    "    feature_importances['permutation'] = perm_importances\n",
    "    feature_importances['permutation_std'] = result.importances_std\n",
    "    feature_importances.to_csv(feature_importances_fn, index=True)\n",
    "    print('Feature importances saved to file:', feature_importances_fn)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "    mdi_importances.plot.bar(yerr=std, ax=ax[0])\n",
    "    ax[0].set_title(\"Feature importances via MDI\")\n",
    "    ax[0].set_ylabel(\"Mean decrease in impurity\")\n",
    "    perm_importances.plot.bar(yerr=result.importances_std, ax=ax[1])\n",
    "    ax[1].set_title(\"Feature importances via permutation\")\n",
    "    ax[1].set_ylabel(\"Mean accuracy decrease\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_fn = os.path.join(out_dir, 'feature_importances.png')\n",
    "    fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "    \n",
    "else:\n",
    "    feature_importances = pd.read_csv(feature_importances_fn)\n",
    "    print('Feature importances loaded from file.')\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use best model to predict snow depth at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modeled snow depth already exists\n",
    "sd_pred_fn = os.path.join(out_dir, f'modeled_snow_depth_{site_name}_{date}.tif')\n",
    "if not os.path.exists(sd_pred_fn):\n",
    "    # Load SkySat snow depth\n",
    "    print('Loading input files')\n",
    "    sd = xdem.DEM(sd_fn)\n",
    "    refdem = xdem.DEM(refdem_fn).reproject(sd)\n",
    "    with open(terrain_dict_fn, 'r') as f:\n",
    "        terrain_dict = json.load(f)\n",
    "    for key, value in terrain_dict.items():\n",
    "        terrain_dict[key] = float(value)\n",
    "    \n",
    "    # Construct features (terrain parameters)\n",
    "    elev, slope, aspect, tpi, sx = pprocess.calculate_terrain_params(refdem, terrain_dict, sd)\n",
    "    \n",
    "    # Identify real value indices (for reshaping results later)\n",
    "    print('Constructing features')\n",
    "    rasters = [elev, slope, aspect, tpi, sx]\n",
    "    ix = [np.where((np.isfinite(raster.data) & ~np.isnan(raster.data)), True, False) \n",
    "        for raster in rasters]\n",
    "    ireal = np.full(sd.shape, True)\n",
    "    for ixx in ix:\n",
    "        ireal = ireal & ixx\n",
    "    # create df of raster values\n",
    "    df = pd.DataFrame({'elevation': elev.data[ireal].ravel(),\n",
    "                       'slope': slope.data[ireal].ravel(),\n",
    "                       'aspect': aspect.data[ireal].ravel(),\n",
    "                       'topographic_position_index': tpi.data[ireal].ravel(),\n",
    "                       'Sx': sx.data[ireal].ravel()})\n",
    "    df = df.round(2) # round values\n",
    "\n",
    "    # Scale the input features\n",
    "    df = scaler.transform(df)\n",
    "\n",
    "    # Predict snow depth\n",
    "    print('Modeling snow depth')\n",
    "    y_pred = best_model.predict(df)\n",
    "\n",
    "    # Reshape into the raster shape\n",
    "    snow_depth_pred = np.full(np.shape(ireal), np.nan)\n",
    "    snow_depth_pred[ireal] = y_pred\n",
    "\n",
    "    # Reformat as xdem.DEM\n",
    "    sd_pred = xdem.DEM.from_array(snow_depth_pred, transform=sd.transform, crs=sd.crs, nodata=np.nan)\n",
    "    \n",
    "    # Save to file\n",
    "    sd_pred.save(sd_pred_fn)\n",
    "    print('Modeled snow depth saved to file:', sd_pred_fn)\n",
    "    \n",
    "    # Compare to observations\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "    ax = ax.flatten()\n",
    "    sd_pred.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[0], add_cbar=False)\n",
    "    ax[0].set_title('Modeled')\n",
    "    sd.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[1], cbar_title='Snow depth [m]')\n",
    "    ax[1].set_title('Observed')\n",
    "    diff = sd_pred - sd\n",
    "    diff.plot(cmap='coolwarm_r', vmin=-5, vmax=5, ax=ax[2], cbar_title='Difference [m]')\n",
    "    ax[2].set_title('Modeled - Observed')\n",
    "    for axis in ax[0:3]:\n",
    "        axis.set_xticks([])\n",
    "        axis.set_yticks([])\n",
    "    ax[3].hist(diff.data.ravel(), bins=np.arange(-5, 5, step=0.2), facecolor='skyblue', edgecolor='k', linewidth=0.5)\n",
    "    ax[3].set_xlabel('Modeled - Observed [m]')\n",
    "    ax[3].set_ylabel('Frequency')\n",
    "    ax[3].set_yticks([])\n",
    "    ax[3].axvline(0, color='k', linewidth=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure to file\n",
    "    fig_fn = os.path.splitext(sd_pred_fn)[0] + '.png'\n",
    "    fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "\n",
    "else:\n",
    "    print('Modeled snow depth already exists, skipping.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use best model to predict snow depth at Mores Creek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modeled snow depth already exists\n",
    "sd_pred_mcs_fn = os.path.join(out_dir, 'modeled_snow_depth_extrapMCS.tif')\n",
    "if not os.path.exists(sd_pred_mcs_fn):\n",
    "    # Load training and validation data at MCS\n",
    "    print('Loading training and validation data')\n",
    "    sd_lidar_fn = os.path.join(data_dir, 'MCS', 'SNEX_MCS_Lidar', 'SNEX_MCS_Lidar_20240418_SD_V01.0.tif')\n",
    "    sd_lidar = xdem.DEM(sd_lidar_fn).reproject(res=[1,1])\n",
    "    if site_name=='MCS':\n",
    "        mcs_refdem_fn = refdem_fn\n",
    "    else:\n",
    "        mcs_refdem_fn = os.path.join(data_dir, 'MCS', 'refdem', 'MCS_REFDEM_WGS84.tif')\n",
    "    mcs_refdem = xdem.DEM(mcs_refdem_fn).reproject(sd_lidar, nodata=np.nan)\n",
    "    mcs_terrain_dict_fn = os.path.join(data_dir, 'MCS', 'refdem', 'refdem_terrain_sampling_resolutions.json')\n",
    "    with open(mcs_terrain_dict_fn, 'r') as f:\n",
    "        mcs_terrain_dict = json.load(f)\n",
    "    for key, value in mcs_terrain_dict.items():\n",
    "        mcs_terrain_dict[key] = float(value)\n",
    "\n",
    "    # Construct features (terrain parameters)\n",
    "    elev, slope, aspect, tpi, sx = pprocess.calculate_terrain_params(mcs_refdem, mcs_terrain_dict, sd_lidar)\n",
    "    \n",
    "    # Identify real value indices (for reshaping results later)\n",
    "    print('Constructing features')\n",
    "    rasters = [elev, slope, aspect, tpi, sx]\n",
    "    ix = [np.where((np.isfinite(raster.data) & ~np.isnan(raster.data)), True, False) \n",
    "        for raster in rasters]\n",
    "    ireal = np.full(sd_lidar.shape, True)\n",
    "    for ixx in ix:\n",
    "        ireal = ireal & ixx\n",
    "\n",
    "    # create df of raster values\n",
    "    df = pd.DataFrame({'elevation': elev.data[ireal].ravel(),\n",
    "                    'slope': slope.data[ireal].ravel(),\n",
    "                    'aspect': aspect.data[ireal].ravel(),\n",
    "                    'topographic_position_index': tpi.data[ireal].ravel(),\n",
    "                    'Sx': sx.data[ireal].ravel()})\n",
    "    df = df.round(2) # round values\n",
    "\n",
    "    # Scale the input features\n",
    "    df = scaler.transform(df)\n",
    "\n",
    "    # Predict snow depth\n",
    "    print('Modeling snow depth')\n",
    "    y_pred = best_model.predict(df)\n",
    "\n",
    "    # Reshape into the raster shape\n",
    "    snow_depth_pred = np.full(np.shape(ireal), np.nan)\n",
    "    snow_depth_pred[ireal] = y_pred\n",
    "\n",
    "    # Reformat as xdem.DEM\n",
    "    sd_pred_mcs = xdem.DEM.from_array(snow_depth_pred, transform=sd_lidar.transform, crs=sd_lidar.crs, nodata=np.nan)\n",
    "    \n",
    "    # Save to file\n",
    "    sd_pred_mcs.save(sd_pred_mcs_fn)\n",
    "    print('Modeled snow depth at MCS saved to file:', sd_pred_mcs_fn)\n",
    "    \n",
    "    # Compare to lidar observations\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "    ax = ax.flatten()\n",
    "    sd_pred_mcs.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[0], add_cbar=False)\n",
    "    ax[0].set_title('Modeled')\n",
    "    sd_lidar.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[1], cbar_title='Snow depth [m]')\n",
    "    ax[1].set_title('Lidar')\n",
    "    diff = sd_pred_mcs - sd_lidar\n",
    "    diff.plot(cmap='coolwarm_r', vmin=-5, vmax=5, ax=ax[2], cbar_title='Difference [m]')\n",
    "    ax[2].set_title('Modeled - Lidar')\n",
    "    for axis in ax[0:3]:\n",
    "        axis.set_xticks([])\n",
    "        axis.set_yticks([])\n",
    "    ax[3].hist(diff.data.ravel(), bins=np.arange(-5, 5, step=0.2), facecolor='skyblue', edgecolor='k', linewidth=0.5)\n",
    "    ax[3].set_xlabel('Modeled - Lidar [m]')\n",
    "    ax[3].set_ylabel('Frequency')\n",
    "    ax[3].set_yticks([])\n",
    "    ax[3].axvline(0, color='k', linewidth=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure to file\n",
    "    fig_fn = os.path.splitext(sd_pred_mcs_fn)[0] + '.png'\n",
    "    fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "\n",
    "else:\n",
    "    print('Modeled snow depth at MCS already exists, skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skysat_stereo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
