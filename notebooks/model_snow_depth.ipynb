{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Model snow depth using terrain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import xdem\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = '/Users/raineyaberle/Research/PhD/SnowDEMs/skysat_stereo_snow/skysat_stereo_snow'\n",
    "data_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/SkySat-Stereo/study-sites'\n",
    "\n",
    "# Inputs\n",
    "site_name = \"MCS\"\n",
    "date = \"20240420\"\n",
    "if site_name=='MCS':\n",
    "    sd_fn = os.path.join(data_dir, site_name, date, 'post_process', f\"coregAll_ba-u5m_{site_name}_{date}_DEM_GCPshift_snow_depth.tif\")\n",
    "else:\n",
    "    sd_fn = os.path.join(data_dir, site_name, date, 'post_process', f\"{site_name}_{date}_DEM_GCPshift_snow_depth.tif\")    \n",
    "# sd_fn = os.path.join(data_dir, site_name, 'SNEX_QSI_SD', 'SNEX20_QSI_SD_0.5M_USIDBS_20210315_20210315.tif')\n",
    "if site_name==\"JacksonPeak\":\n",
    "    refdem_fn = os.path.join(data_dir, site_name, 'refdem', 'USGS_LPC_ID_FEMAHQ_2018_D18_merged_filtered_UTM11_filled.tif')\n",
    "else:\n",
    "    refdem_fn = os.path.join(data_dir, site_name, 'refdem', f\"{site_name}_REFDEM_WGS84.tif\")\n",
    "\n",
    "# Determine whether to scale input features\n",
    "scale_inputs = False\n",
    "\n",
    "# Outputs\n",
    "out_dir = os.path.join(data_dir, site_name, date, 'snow_depth_modeling')\n",
    "# out_dir = os.path.join(os.path.dirname(sd_fn), 'snow_depth_modeling')\n",
    "training_data_fn = os.path.join(out_dir, 'training_data.csv')\n",
    "error_fn = os.path.join(out_dir, 'model_error.csv')\n",
    "model_fn = os.path.join(out_dir, 'trained_model.joblib')\n",
    "\n",
    "# Check that inputs exist\n",
    "for fn, name in [[sd_fn, 'Snow depth map'], [refdem_fn, 'Reference DEM']]:\n",
    "    if not os.path.exists(fn):\n",
    "        print(f\"{name} not found, please correct before continuing.\")\n",
    "# Create output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# Import processing functions\n",
    "sys.path.append(code_dir)\n",
    "import post_process_utils as pprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data already exist\n",
    "if not os.path.exists(training_data_fn):\n",
    "    ### Load input files\n",
    "    # Snow depth\n",
    "    sd = xdem.DEM(sd_fn).reproject(res=2)\n",
    "    # Terrain parameters\n",
    "    elev = xdem.DEM(refdem_fn.replace('.tif', '_ELEVATION.tif')).reproject(sd)\n",
    "    slope = xdem.DEM(refdem_fn.replace('.tif', '_SLOPE.tif')).reproject(sd)\n",
    "    aspect = xdem.DEM(refdem_fn.replace('.tif', '_ASPECT.tif')).reproject(sd)\n",
    "    tpi = xdem.DEM(refdem_fn.replace('.tif', '_TPI.tif')).reproject(sd)\n",
    "    sx_fn = glob.glob(refdem_fn.replace('.tif', '*Sx*.tif'))[0]\n",
    "    sx = xdem.DEM(sx_fn).reproject(sd)\n",
    "    # Compile into pandas.DataFrame\n",
    "    training_data = pd.DataFrame({'elevation': elev.data.ravel(),\n",
    "                                  'slope': slope.data.ravel(),\n",
    "                                  'aspect': aspect.data.ravel(),\n",
    "                                  'topographic_position_index': tpi.data.ravel(),\n",
    "                                  'Sx': sx.data.ravel(),\n",
    "                                  'snow_depth': sd.data.ravel()})\n",
    "    training_data.dropna(inplace=True)\n",
    "    training_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Reduce precision (don't need 10 digits of elevation, e.g.)\n",
    "    training_data = training_data.round(2)\n",
    "    \n",
    "    # Save to file\n",
    "    training_data.to_csv(training_data_fn, index=False)\n",
    "    print('Training data saved to file:', training_data_fn)\n",
    "    \n",
    "    # Plot pairplot\n",
    "    # fig_fn = os.path.join(out_dir, 'training_data_pairplot.png')\n",
    "    # fig = sns.pairplot(training_data, corner=True, kind='hist', diag_kind='kde')\n",
    "    # fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    # print('Pairplot saved to file:', fig_fn)\n",
    "    # plt.show()\n",
    "\n",
    "else:\n",
    "\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    print('Training data loaded from file.')\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['elevation', 'slope', 'aspect', 'topographic_position_index', 'Sx']\n",
    "target_cols = ['snow_depth']\n",
    "X = training_data[feature_cols]\n",
    "y = training_data[target_cols]\n",
    "\n",
    "if scale_inputs:\n",
    "    # Fit a standard scaler to the feature columns\n",
    "    scaler_fn = os.path.join(out_dir, 'feature_scaler.joblib')\n",
    "    if not os.path.exists(scaler_fn):\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        joblib.dump(scaler, scaler_fn)\n",
    "        print('Feature scaler saved to file:', scaler_fn)\n",
    "    else:\n",
    "        scaler = joblib.load(scaler_fn)\n",
    "        print('Feature scaler loaded from file.')\n",
    "        \n",
    "    # Transform the feature columns\n",
    "    X_scaled = scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, and test the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = os.path.join(out_dir, 'trained_model.joblib')\n",
    "if not os.path.exists(model_fn):\n",
    "\n",
    "    ### Use ~20,000 random points from the training data\n",
    "    nsamp = int(len(X) / 20e3)\n",
    "    if scale_inputs:\n",
    "        X_sub, y_sub = X_scaled.iloc[::nsamp,:], y.iloc[::nsamp,:]\n",
    "    else:\n",
    "        X_sub, y_sub = X.iloc[::nsamp,:], y.iloc[::nsamp,:]\n",
    "    X_sub.reset_index(drop=True, inplace=True)\n",
    "    y_sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ### Set up the hyperparameter grid\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [10] + [int(x) for x in np.linspace(100, 1000, num = 10)]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4, 10]\n",
    "    # Create the random grid\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                  'max_depth': max_depth,\n",
    "                  'min_samples_split': min_samples_split,\n",
    "                  'min_samples_leaf': min_samples_leaf}\n",
    "    print('Hyperparameter Grid:')\n",
    "    print(json.dumps(param_grid, sort_keys=True, indent=4))\n",
    "\n",
    "    ### Search for best hyperparameters\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_sub, y_sub)\n",
    "    print('Best parameters:')\n",
    "    print(rf_random.best_params_)\n",
    "    \n",
    "    ### Compare to the default (base) model\n",
    "    cv_model = rf_random.best_estimator_\n",
    "    y_pred = cv_model.predict(X_sub)\n",
    "    cv_rmse = np.sqrt(mean_squared_error(y_sub, y_pred))\n",
    "    print(f\"Best model RMSE = {np.round(cv_rmse,2)} m\")\n",
    "    \n",
    "    ### Save the best model\n",
    "    best_model = cv_model\n",
    "    joblib.dump(best_model, model_fn)\n",
    "    print('Best model saved to file:', model_fn)\n",
    "\n",
    "else:\n",
    "    best_model = joblib.load(model_fn)\n",
    "    print('Best model loaded from file.')\n",
    "    \n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate feature importances in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_fn = os.path.join(out_dir, 'feature_importances.csv')\n",
    "if not os.path.exists(feature_importances_fn):\n",
    "    # Feature importances via mean decrease in impurity\n",
    "    mdi_importances = best_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in best_model.estimators_], axis=0)\n",
    "    mdi_importances = pd.Series(mdi_importances, index=feature_cols)\n",
    "\n",
    "    # Feature importances via permutation\n",
    "    result = permutation_importance(best_model, X_sub, y_sub, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_importances = pd.Series(result.importances_mean, index=feature_cols)\n",
    "\n",
    "    # Save in dataframe\n",
    "    feature_importances = pd.DataFrame(mdi_importances.values, feature_cols, ['MDI'])\n",
    "    feature_importances['MDI_std'] = std\n",
    "    feature_importances['permutation'] = perm_importances\n",
    "    feature_importances['permutation_std'] = result.importances_std\n",
    "    feature_importances.to_csv(feature_importances_fn, index=True)\n",
    "    print('Feature importances saved to file:', feature_importances_fn)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "    mdi_importances.plot.bar(yerr=std, ax=ax[0])\n",
    "    ax[0].set_title(\"Feature importances via MDI\")\n",
    "    ax[0].set_ylabel(\"Mean decrease in impurity\")\n",
    "    perm_importances.plot.bar(yerr=result.importances_std, ax=ax[1])\n",
    "    ax[1].set_title(\"Feature importances via permutation\")\n",
    "    ax[1].set_ylabel(\"Mean accuracy decrease\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_fn = os.path.join(out_dir, 'feature_importances.png')\n",
    "    fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "    \n",
    "else:\n",
    "    feature_importances = pd.read_csv(feature_importances_fn)\n",
    "    print('Feature importances loaded from file.')\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use best model to predict snow depth at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modeled snow depth already exists\n",
    "sd_pred_fn = os.path.join(out_dir, f'modeled_snow_depth_{site_name}_{date}.tif')\n",
    "if not os.path.exists(sd_pred_fn):\n",
    "    \n",
    "    # Load terrain parameters \n",
    "    print('Loading input files')\n",
    "    elev = xdem.DEM(refdem_fn.replace('.tif', '_ELEVATION.tif')).reproject(res=2)\n",
    "    slope = xdem.DEM(refdem_fn.replace('.tif', '_SLOPE.tif')).reproject(elev)\n",
    "    aspect = xdem.DEM(refdem_fn.replace('.tif', '_ASPECT.tif')).reproject(elev)\n",
    "    tpi = xdem.DEM(refdem_fn.replace('.tif', '_TPI.tif')).reproject(elev)\n",
    "    sx_fn = glob.glob(refdem_fn.replace('.tif', '*Sx*.tif'))[0]\n",
    "    sx = xdem.DEM(sx_fn).reproject(elev)\n",
    "    \n",
    "    # Load SkySat snow depth\n",
    "    sd = xdem.DEM(sd_fn).reproject(elev)\n",
    "    \n",
    "    # Identify real value indices (for reshaping results later)\n",
    "    print('Constructing features')\n",
    "    rasters = [elev, slope, aspect, tpi, sx]\n",
    "    ix = [np.where((np.isfinite(raster.data) & ~np.isnan(raster.data)), True, False) \n",
    "        for raster in rasters]\n",
    "    ireal = np.full(sd.shape, True)\n",
    "    for ixx in ix:\n",
    "        ireal = ireal & ixx\n",
    "    # create df of raster values\n",
    "    df = pd.DataFrame({'elevation': elev.data[ireal].ravel(),\n",
    "                       'slope': slope.data[ireal].ravel(),\n",
    "                       'aspect': aspect.data[ireal].ravel(),\n",
    "                       'topographic_position_index': tpi.data[ireal].ravel(),\n",
    "                       'Sx': sx.data[ireal].ravel()})\n",
    "    df = df.round(2) # round values\n",
    "\n",
    "    # Scale the input features\n",
    "    if scale_inputs:\n",
    "        df = scaler.transform(df)\n",
    "\n",
    "    # Predict snow depth\n",
    "    print('Modeling snow depth')\n",
    "    with joblib.parallel_backend('threading', n_jobs=12):\n",
    "        y_pred = best_model.predict(df)\n",
    "\n",
    "    # Reshape into the raster shape\n",
    "    snow_depth_pred = np.full(np.shape(ireal), np.nan)\n",
    "    snow_depth_pred[ireal] = y_pred\n",
    "\n",
    "    # Reformat as xdem.DEM\n",
    "    sd_pred = xdem.DEM.from_array(snow_depth_pred, transform=sd.transform, crs=sd.crs, nodata=np.nan)\n",
    "    \n",
    "    # Save to file\n",
    "    sd_pred.save(sd_pred_fn)\n",
    "    print('Modeled snow depth saved to file:', sd_pred_fn)\n",
    "    \n",
    "    # Compare to observations\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "    ax = ax.flatten()\n",
    "    sd_pred.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[0], add_cbar=False)\n",
    "    ax[0].set_title('Modeled')\n",
    "    sd.plot(cmap='Blues', vmin=0, vmax=5, ax=ax[1], cbar_title='Snow depth [m]')\n",
    "    ax[1].set_title('Observed')\n",
    "    diff = sd_pred - sd\n",
    "    diff.plot(cmap='coolwarm_r', vmin=-5, vmax=5, ax=ax[2], cbar_title='Difference [m]')\n",
    "    ax[2].set_title('Modeled - Observed')\n",
    "    for axis in ax[0:3]:\n",
    "        axis.set_xticks([])\n",
    "        axis.set_yticks([])\n",
    "    ax[3].hist(diff.data.ravel(), bins=np.arange(-5, 5, step=0.2), facecolor='skyblue', edgecolor='k', linewidth=0.5)\n",
    "    ax[3].set_xlabel('Modeled - Observed [m]')\n",
    "    ax[3].set_ylabel('Frequency')\n",
    "    ax[3].set_yticks([])\n",
    "    ax[3].axvline(0, color='k', linewidth=1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure to file\n",
    "    fig_fn = os.path.splitext(sd_pred_fn)[0] + '.png'\n",
    "    fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "\n",
    "else:\n",
    "    print('Modeled snow depth already exists, skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skysat_stereo_snow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
